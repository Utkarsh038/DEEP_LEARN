{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2ZHWEfScxzweZ1k1sMjNv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsh038/DEEP_LEARN/blob/main/BASIC_ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Artificial Neurons\n",
        "An artificial neuron mimics the behavior of a biological neuron using mathematical computations.\n",
        "\n",
        "Example 1:\n",
        "\n",
        "Biological Neuron vs. Artificial Neuron\n",
        "\n",
        "A biological neuron takes input signals through dendrites, processes them, and sends an output through axons. Similarly, an artificial neuron takes numerical inputs, applies weights, adds bias, and passes through an activation function."
      ],
      "metadata": {
        "id": "Xxy_MXR28TBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Artificial Neuron Example\n",
        "inputs = np.array([0.5, 0.3])  # Example input signals\n",
        "weights = np.array([0.8, 0.2])  # Weights\n",
        "bias = 0.1  # Bias\n",
        "\n",
        "# Weighted sum computation\n",
        "output = np.dot(inputs, weights) + bias\n",
        "print(\"Neuron Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-iF6AXo8UGO",
        "outputId": "2ab3f255-df02-4a1f-a94e-0e5cfb31bde7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neuron Output: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2: Activation Functions\n",
        "Let's apply different activation functions to the output of an artificial neuron."
      ],
      "metadata": {
        "id": "-IdloZXL82PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation Functions\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Stability improvement\n",
        "    return exp_x / exp_x.sum()\n",
        "\n",
        "x = np.array([1.0, 2.0, -1.0])\n",
        "\n",
        "print(\"Sigmoid:\", sigmoid(x))\n",
        "print(\"ReLU:\", relu(x))\n",
        "print(\"Tanh:\", tanh(x))\n",
        "print(\"Softmax:\", softmax(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjLB1TPD86Ee",
        "outputId": "ed680dbb-49b7-4c2e-cbae-e9b08cd7b6cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid: [0.73105858 0.88079708 0.26894142]\n",
            "ReLU: [1. 2. 0.]\n",
            "Tanh: [ 0.76159416  0.96402758 -0.76159416]\n",
            "Softmax: [0.25949646 0.70538451 0.03511903]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Computational Models of Neurons\n",
        "An artificial neuron follows this equation:\n",
        "\n",
        "\n",
        "    \n",
        "ùë¶\n",
        "=\n",
        "ùëì\n",
        "(\n",
        "‚àë\n",
        "(\n",
        "ùë§\n",
        "ùëñ\n",
        "ùë•\n",
        "ùëñ\n",
        ")\n",
        "+\n",
        "ùëè\n",
        ")\n",
        "y=f(‚àë(w\n",
        "i\n",
        "‚Äã\n",
        " x\n",
        "i\n",
        "‚Äã\n",
        " )+b)\n",
        "\n",
        "\n",
        "\n",
        "Example 3: Weighted Sum and Bias"
      ],
      "metadata": {
        "id": "RqvVPh_d9Caf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array([0.2, 0.8, -0.5])\n",
        "weights = np.array([0.3, -0.2, 0.5])\n",
        "bias = 0.1\n",
        "\n",
        "# Compute weighted sum\n",
        "weighted_sum = np.dot(inputs, weights) + bias\n",
        "print(\"Weighted Sum:\", weighted_sum)\n"
      ],
      "metadata": {
        "id": "3Etrsuvr9Oon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 4: Gradient Descent Intuition\n",
        "Gradient descent updates weights to minimize loss in machine learning."
      ],
      "metadata": {
        "id": "A7_vxeAw-MAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(learning_rate=0.1, epochs=10):\n",
        "    w = 5  # Initial weight\n",
        "    for i in range(epochs):\n",
        "        gradient = 2 * w  # Derivative of cost function (w^2)\n",
        "        w = w - learning_rate * gradient  # Update weight\n",
        "        print(f\"Epoch {i+1}: w = {w}\")\n",
        "\n",
        "gradient_descent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B75Awhy4-Qov",
        "outputId": "4e7bef45-8eae-4cb1-ccb3-0cbd46f9f017"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: w = 4.0\n",
            "Epoch 2: w = 3.2\n",
            "Epoch 3: w = 2.56\n",
            "Epoch 4: w = 2.048\n",
            "Epoch 5: w = 1.6384\n",
            "Epoch 6: w = 1.31072\n",
            "Epoch 7: w = 1.0485760000000002\n",
            "Epoch 8: w = 0.8388608000000002\n",
            "Epoch 9: w = 0.6710886400000001\n",
            "Epoch 10: w = 0.5368709120000001\n"
          ]
        }
      ]
    }
  ]
}